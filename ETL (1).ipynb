{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deef8eee-2b4e-468d-8fb2-26219c8916a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Extracting the Data from the S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08f0b1e0-c2c0-465f-916e-cd711e967240",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/s3_mount has been unmounted.\n"
     ]
    }
   ],
   "source": [
    "dbutils.fs.unmount(\"/mnt/s3_mount\")\n",
    "aws_access_key = dbutils.widgets.text(\"AWS Access Key\", \"\")\n",
    "aws_secret_key = dbutils.widgets.text(\"AWS Secret Key\", \"\")\n",
    "\n",
    "# Getting the values through widgets\n",
    "aws_access_key_value = dbutils.widgets.get(\"AWS Access Key\")\n",
    "aws_secret_key_value = dbutils.widgets.get(\"AWS Secret Key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a34bd7b-c6ac-4dbc-b823-429dd724d9e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[106]: True"
     ]
    }
   ],
   "source": [
    "s3_bucket_name = \"retail90\"\n",
    "mount_name = \"s3_mount\"\n",
    "# Use the  access and secret key generated from the AWS account\n",
    "dbutils.fs.mount(\n",
    "    source=f\"s3a://{s3_bucket_name}\",\n",
    "    mount_point=f\"/mnt/{mount_name}\",\n",
    "    extra_configs={\n",
    "        \"fs.s3a.access.key\": aws_access_key_value,\n",
    "        \"fs.s3a.secret.key\": aws_secret_key_value\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc952421-61b5-4d24-86dd-26ff25a5a76a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/s3_mount/Processed/</td><td>Processed/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/s3_mount/Staging/</td><td>Staging/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/s3_mount/Processed/",
         "Processed/",
         0,
         0
        ],
        [
         "dbfs:/mnt/s3_mount/Staging/",
         "Staging/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/%s\" % mount_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71a7ebfd-6d50-40f5-b8de-4f90d481909a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Transform the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af92be9e-4b0b-4778-9a47-126fe869ba82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3158566027003500#setting/sparkui/0909-141042-33udt4aa/driver-7813382985247891276\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3158566027003500#setting/sparkui/0909-141042-33udt4aa/driver-7813382985247891276\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc0c895-eead-44be-9d40-2cbd83c86c06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "# Online Store Database Schema by manually defining the schema\n",
    "online_store_schema= StructType([\n",
    "    StructField(\"cust_id\", IntegerType(), True),  \n",
    "    StructField(\"date\", DateType(), True),        \n",
    "    StructField(\"age\", IntegerType(), True),      \n",
    "    StructField(\"gender\", StringType(), True),    \n",
    "    StructField(\"item\", StringType(), True),      \n",
    "    StructField(\"quantity\", IntegerType(), True), \n",
    "    StructField(\"amount\", FloatType(), True),     \n",
    "    StructField(\"discount\", StringType(), True),  \n",
    "    StructField(\"rating\", IntegerType(), True),   \n",
    "    StructField(\"transaction_id\", StringType(), True), \n",
    "    StructField(\"location\", StringType(), True)   \n",
    "])\n",
    "# Reading the schema of csv format\n",
    "online_store= spark.read.schema(online_store_schema).format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/mnt/s3_mount/Staging/online_store_schema.csv\")\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf4a9003-8d50-4a01-8849-53025af05785",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Physical Store POS System Schema\n",
    "pos_system_schema = StructType([\n",
    "    StructField(\"cust_id\", IntegerType(), True),  \n",
    "    StructField(\"date\", DateType(), True),        \n",
    "    StructField(\"age\", IntegerType(), True),      \n",
    "    StructField(\"gender\", StringType(), True),    \n",
    "    StructField(\"item\", StringType(), True),     \n",
    "    StructField(\"quantity\", IntegerType(), True), \n",
    "    StructField(\"amount\", FloatType(), True),     \n",
    "    StructField(\"discount\", StringType(), True),  # Discount expecting string in percentage format\n",
    "    StructField(\"rating\", IntegerType(), True),   \n",
    "    StructField(\"transaction_id\", StringType(), True), \n",
    "    StructField(\"location\", StringType(), True)   \n",
    "])\n",
    "pos_system= spark.read.schema(pos_system_schema).format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/mnt/s3_mount/Staging/pos_system_schema.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d64d4b7-475f-4637-a8fb-1b395e87d0a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loyalty Program Database Schema\n",
    "loyalty_program_schema = StructType([\n",
    "    StructField(\"cust_id\", IntegerType(), True),  \n",
    "    StructField(\"date\", DateType(), True),        \n",
    "    StructField(\"age\", IntegerType(), True),      \n",
    "    StructField(\"gender\", StringType(), True),    \n",
    "    StructField(\"item\", StringType(), True),      \n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"amount\", FloatType(), True),     \n",
    "    StructField(\"discount\", StringType(), True),  \n",
    "    StructField(\"rating\", IntegerType(), True),   \n",
    "    StructField(\"transaction_id\", StringType(), True), \n",
    "    StructField(\"location\", StringType(), True)   \n",
    "])\n",
    "loyalty_program= spark.read.schema(loyalty_program_schema).format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/mnt/s3_mount/Staging/loyalty_program_schema.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b02ebeb8-742d-4528-8912-350f62cbe2a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[112]: [Row(cust_id=332, date=datetime.date(2023, 6, 26), age=39, gender='Female', item='Accessories', quantity=2, amount=3854.830078125, discount='90%', rating=2, transaction_id='be169d6b-f290-4831-852d-143c23a61526', location='California'),\n Row(cust_id=158, date=datetime.date(2023, 10, 13), age=120, gender='Female', item='Clothing', quantity=2, amount=10000.0, discount='-10%', rating=3, transaction_id='1a0408e7-cbab-45d7-a28c-77158f8ac4cd', location='N/A'),\n Row(cust_id=364, date=datetime.date(2023, 11, 5), age=120, gender='Female', item='Grocery', quantity=10, amount=10000.0, discount='-10%', rating=1, transaction_id='9b86d80d-10b8-4060-b580-d3a313ab42c4', location='Unknown'),\n Row(cust_id=340, date=datetime.date(2023, 5, 15), age=20, gender='Female', item='Accessories', quantity=1, amount=318.92999267578125, discount='54%', rating=5, transaction_id='33a70c3d-1da5-4563-a939-e190c6f1c3ad', location='Florida'),\n Row(cust_id=92, date=datetime.date(2023, 5, 6), age=37, gender='Male', item='Electronics', quantity=6, amount=2017.510009765625, discount='6%', rating=3, transaction_id='7cbb1bcc-ef38-4237-99a0-f532d2197d93', location='California')]"
     ]
    }
   ],
   "source": [
    "loyalty_program.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a2d147-0647-4871-960a-c5d34b792de2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial columns: ['cust_id', 'date', 'age', 'gender', 'item', 'quantity', 'amount', 'discount', 'rating', 'transaction_id', 'location']\nColumns being selected: ['cust_id', 'date', 'age', 'gender', 'item', 'quantity', 'amount', 'discount', 'rating', 'transaction_id', 'location']\nRows after removing invalid locations: 573\nMode item calculated: Clothing\nRows after filling missing values: 573\nMean age calculated: 46\nFinal row count: 573\nInitial columns: ['cust_id', 'date', 'age', 'gender', 'item', 'quantity', 'amount', 'discount', 'rating', 'transaction_id', 'location']\nColumns being selected: ['cust_id', 'date', 'age', 'gender', 'item', 'quantity', 'amount', 'discount', 'rating', 'transaction_id', 'location']\nRows after removing invalid locations: 748\nMode item calculated: Electronics\nRows after filling missing values: 748\nMean age calculated: 44\nFinal row count: 748\nInitial columns: ['cust_id', 'date', 'age', 'gender', 'item', 'quantity', 'amount', 'discount', 'rating', 'transaction_id', 'location']\nColumns being selected: ['cust_id', 'date', 'age', 'gender', 'item', 'quantity', 'amount', 'discount', 'rating', 'transaction_id', 'location']\nRows after removing invalid locations: 373\nMode item calculated: Clothing\nRows after filling missing values: 373\nMean age calculated: 43\nFinal row count: 373\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import col, when, regexp_replace, count, isnan\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Transform DataFrame Function\n",
    "def transformation(df):\n",
    "    MANDATORY_COLS = ['cust_id', 'date', 'age', 'gender', 'item', 'quantity', 'amount', 'discount', 'rating', 'transaction_id', 'location']\n",
    "\n",
    "    # Checking  if all the  mandatory columns exist\n",
    "    print(\"Initial columns:\", df.columns)\n",
    "    existing_cols = [col for col in MANDATORY_COLS if col in df.columns]\n",
    "    print(\"Columns being selected:\", existing_cols)\n",
    "    df = df.select(existing_cols)\n",
    "    \n",
    "    # Removing invalid locations\n",
    "    invalid_locations = ['Unknown', 'N/A', 'Invalid Location']\n",
    "    df = df.filter(~F.col('location').isin(invalid_locations))\n",
    "    print(\"Rows after removing invalid locations:\", df.count())\n",
    "\n",
    "    # Calculate mode item (handle NoneType issues)\n",
    "    mode_item_row = df.groupBy('item').count().orderBy(F.desc('count')).first()\n",
    "    mode_item = mode_item_row['item'] if mode_item_row else None\n",
    "    print(f\"Mode item calculated: {mode_item}\")\n",
    "\n",
    "    #  Fill missing values\n",
    "    default_values = {'quantity': 1, 'amount': 0, 'discount': 0, 'rating': 3}\n",
    "    df = df.fillna(default_values)\n",
    "    print(\"Rows after filling missing values:\", df.count())\n",
    "\n",
    "    #  Handle missing or invalid ages\n",
    "    mean_age = df.select(F.mean('age')).first()[0] or 30  # Default to 30 if mean_age is None\n",
    "    mean_age=round(mean_age)\n",
    "    print(f\"Mean age calculated: {mean_age}\")\n",
    "\n",
    "    df = df.withColumn('age', F.when(F.col('age').isNull(), mean_age).otherwise(F.col('age')))\n",
    "    df = df.withColumn('age', F.when(F.col('age').between(18, 100), F.col('age')).otherwise(mean_age))\n",
    "\n",
    "    # Replace missing 'item' with mode item if mode exists\n",
    "    if mode_item:\n",
    "        df = df.withColumn('item', F.when(F.col('item').isNull(), mode_item).otherwise(F.col('item')))\n",
    "    \n",
    "    # Normalize 'amount' and 'discount' fields by removing % symbol also adjusting values from 0 to 100\n",
    "    df = df.withColumn('amount', when(col('amount') < 0, 0).otherwise(col('amount')))\n",
    "    # In regex replacing the % symbol with blank and converting to float\n",
    "    df = df.withColumn('discount', regexp_replace(col('discount'), r'%', '').cast('float'))\n",
    "    df = df.withColumn('discount', when(col('discount') < 0, 0).when(col('discount') > 100, 100).otherwise(col('discount')))\n",
    "    \n",
    "    # Format 'date' column to correct date format\n",
    "    df = df.withColumn('date', F.to_date(col('date'), 'yyyy-MM-dd'))\n",
    "\n",
    "    print(\"Final row count:\", df.count())\n",
    "    return df\n",
    "\n",
    "# Applying transformations on all 3 dataframe\n",
    "transformed_online_store_df = transformation(online_store)\n",
    "transformed_pos_system_df = transformation(pos_system)\n",
    "transformed_loyalty_program_df = transformation(loyalty_program)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a41d81-e59f-4d20-b8fd-effc06fd9ac7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Adding a new column 'Source' with the respective value of the source obtained from \n",
    "transformed_online_store_df = transformed_online_store_df.withColumn(\"Source\", lit(\"Online Store\"))\n",
    "\n",
    "transformed_pos_system_df = transformed_pos_system_df.withColumn(\"Source\", lit(\"Physical store\"))\n",
    "\n",
    "transformed_loyalty_program_df = transformed_loyalty_program_df.withColumn(\"Source\", lit(\"Loyalty Program\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c6c24f0-d569-4f90-b0cf-a60190892ae1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Loading the data again to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a66ae8db-c1db-4a03-82f3-2c896125d0e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Parquet file successfully saved to S3.\n"
     ]
    }
   ],
   "source": [
    "combined_df = transformed_online_store_df.union(transformed_pos_system_df).union(transformed_loyalty_program_df)\n",
    "\n",
    "combined_df_coalesced = combined_df.coalesce(1)\n",
    "\n",
    "# Defining the S3 path where the combined parquet file will be stored\n",
    "output_path = \"dbfs:/mnt/s3_mount/Processed/\"\n",
    "\n",
    "# Write the combined DataFrame as a single Parquet file to S3\n",
    "combined_df_coalesced.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(\"Combined Parquet file successfully saved to S3.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL",
   "widgets": {
    "AWS Access Key": {
     "currentValue": "",
     "nuid": "f45f552d-a227-4634-b830-72c2f3961abc",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "AWS Access Key",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "AWS Secret Key": {
     "currentValue": "",
     "nuid": "ba54a485-4ae3-4c2d-ac54-920aa43fdb59",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "AWS Secret Key",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
